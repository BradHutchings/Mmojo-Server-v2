## 400. Build gguf Models
### About this Section
Let's use llama.cpp and Hugging Face to build some .gguf models we can use with Mmojo Server.

If you have already prepared your build environment, skip ahead to: 
- [500. Build Mmojo Server](500-Build-Mmojo-Server.md)

---
### Convert Models
Here are the things you need to do:

- [401. Prepare to Connvert](401-Prepare-to-Convert.md) -
- [402. Google Gemma](402-Google-Gemma.md) - Clone Google's Gemma Repos on Hugging Face, convert to `.gguf` models.
- [403. Meta Llama](403-Meta-Llama.md) - Clone Meta's Llama Repos on Hugging Face, convert to `.gguf` models.
- [404. Mistral AI Ministral](404-Mistral-AI-Ministral.md) - Clone Mistral AI's Ministral Repos on Hugging Face, convert to `.gguf` models.

**Get Started:** [401. Prepare to Connvert](401-Prepare-to-Convert.md)



---
[MIT License](/LICENSE)<br/>
Copyright (c) 2025 [Brad Hutchings](mailto:brad@bradhutchings.com)<br/>
[https://github.com/BradHutchings/Mmojo-Server](https://github.com/BradHutchings/Mmojo-Server)
